{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import *\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SymbolDetector()\n",
    "model_path = 'resized_model_50.pth'\n",
    "# model_path = 'resized_model_10.pth'\n",
    "# model_path = 'quantized_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path))\n",
    "model = model.cuda()\n",
    "print(\"Model loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_test(model, resize=False):\n",
    "    model.eval()\n",
    "    # Traverse the 'test_data' directory\n",
    "    test_dir = 'test_data'\n",
    "    if resize:\n",
    "        test_dir = 'resized_test_data'\n",
    "    for root, dirs, files in os.walk(test_dir):\n",
    "        for file in files:\n",
    "            image_path = os.path.join(root, file)\n",
    "            image_size = 256\n",
    "            image = Image.open(image_path)\n",
    "            # image = image.convert('RGB')\n",
    "            original_size = image.size\n",
    "            transform = transforms.Compose([\n",
    "                    transforms.Resize((image_size, image_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            image = transform(image).unsqueeze(0).cuda()\n",
    "            with torch.no_grad():\n",
    "                presence_pred, bbox_pred = model(image)\n",
    "                print(f'Presence_pred{presence_pred}, Bbox_pred{bbox_pred}')\n",
    "\n",
    "\n",
    "            threshold = 0.5\n",
    "            symbol_present = (presence_pred > threshold).float()\n",
    "            print(f'Symbol Present: {symbol_present}')\n",
    "\n",
    "            image_cv = cv2.imread(image_path)\n",
    "            # image_cv = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for visualization\n",
    "            image_cv = np.array(image_cv)\n",
    "\n",
    "            if symbol_present.item() == 1:\n",
    "                prediced_boxes = bbox_pred[0].cpu().numpy()\n",
    "                # Extract bounding box coordinates\n",
    "                x_min, y_min, x_max, y_max = prediced_boxes\n",
    "                if resize:\n",
    "                    ...\n",
    "                else:\n",
    "                    x_min = x_min * original_size[0] / image_size\n",
    "                    y_min = y_min * original_size[1] / image_size\n",
    "                    x_max = x_max * original_size[0] / image_size\n",
    "                    y_max = y_max * original_size[1] / image_size\n",
    "                print(f\"Bounding Box Coordinates: {x_min, y_min, x_max, y_max}\")\n",
    "\n",
    "                # Draw the bounding box on the image\n",
    "                cv2.rectangle(image_cv, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 0, 0), 2)\n",
    "                cv2.imshow('Augmented Image', image_cv)\n",
    "                cv2.waitKey(0)\n",
    "                cv2.destroyAllWindows()\n",
    "            else:\n",
    "                center = (original_size[0] // 2, original_size[1] // 2)\n",
    "                cv2.putText(image_cv, \"No Symbol Detected\", center, cv2.FONT_HERSHEY_SIMPLEX, 1, (125, 0, 0), 2)\n",
    "                cv2.imshow('Augmented Image', image_cv)\n",
    "                cv2.waitKey(0)\n",
    "                cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presence_predtensor([[0.9999]], device='cuda:0'), Bbox_predtensor([[ 12.9686,  26.0681,  64.3862, 110.6626]], device='cuda:0')\n",
      "Symbol Present: tensor([[1.]], device='cuda:0')\n",
      "Bounding Box Coordinates: (52.482107400894165, 65.78131729364395, 260.56302642822266, 279.2501680254936)\n",
      "Presence_predtensor([[1.0000]], device='cuda:0'), Bbox_predtensor([[249.2210, 227.7156, 199.2603, 154.2418]], device='cuda:0')\n",
      "Symbol Present: tensor([[1.]], device='cuda:0')\n",
      "Bounding Box Coordinates: (1008.5661444664001, 574.6260242462158, 806.3816487789154, 389.2195574045181)\n",
      "Presence_predtensor([[3.7636e-12]], device='cuda:0'), Bbox_predtensor([[189.8237, 128.8186, 163.3060, 127.4560]], device='cuda:0')\n",
      "Symbol Present: tensor([[0.]], device='cuda:0')\n",
      "Presence_predtensor([[3.3830e-12]], device='cuda:0'), Bbox_predtensor([[191.0873, 132.3286, 163.9569, 128.1853]], device='cuda:0')\n",
      "Symbol Present: tensor([[0.]], device='cuda:0')\n",
      "Presence_predtensor([[9.4987e-12]], device='cuda:0'), Bbox_predtensor([[188.7850, 149.3979, 162.7921, 132.0364]], device='cuda:0')\n",
      "Symbol Present: tensor([[0.]], device='cuda:0')\n",
      "Presence_predtensor([[8.7187e-12]], device='cuda:0'), Bbox_predtensor([[187.5817, 135.5899, 162.5484, 129.6923]], device='cuda:0')\n",
      "Symbol Present: tensor([[0.]], device='cuda:0')\n",
      "Presence_predtensor([[1.6384e-11]], device='cuda:0'), Bbox_predtensor([[157.2824, 167.1162, 146.7599, 137.8593]], device='cuda:0')\n",
      "Symbol Present: tensor([[0.]], device='cuda:0')\n",
      "Presence_predtensor([[6.0593e-06]], device='cuda:0'), Bbox_predtensor([[ 52.3184, 219.4039,  85.8273, 144.5432]], device='cuda:0')\n",
      "Symbol Present: tensor([[0.]], device='cuda:0')\n",
      "Presence_predtensor([[3.5661e-05]], device='cuda:0'), Bbox_predtensor([[220.7565,  28.6708, 180.0286, 107.4026]], device='cuda:0')\n",
      "Symbol Present: tensor([[0.]], device='cuda:0')\n",
      "Presence_predtensor([[1.1624e-05]], device='cuda:0'), Bbox_predtensor([[ 39.3378, 222.8729,  78.7603, 145.3448]], device='cuda:0')\n",
      "Symbol Present: tensor([[0.]], device='cuda:0')\n",
      "Presence_predtensor([[3.7443e-06]], device='cuda:0'), Bbox_predtensor([[224.0569, 207.3946, 181.9767, 144.5085]], device='cuda:0')\n",
      "Symbol Present: tensor([[0.]], device='cuda:0')\n",
      "Presence_predtensor([[3.4853e-06]], device='cuda:0'), Bbox_predtensor([[ 46.2611, 220.0309,  82.3176, 144.3220]], device='cuda:0')\n",
      "Symbol Present: tensor([[0.]], device='cuda:0')\n",
      "Presence_predtensor([[1.0000]], device='cuda:0'), Bbox_predtensor([[ 12.8055,  25.2614,  63.9016, 110.1439]], device='cuda:0')\n",
      "Symbol Present: tensor([[1.]], device='cuda:0')\n",
      "Bounding Box Coordinates: (51.82226565480232, 63.74545392394066, 258.6017407178879, 277.94116562604904)\n",
      "Presence_predtensor([[0.9999]], device='cuda:0'), Bbox_predtensor([[ 12.8127,  26.5654,  64.3641, 110.5576]], device='cuda:0')\n",
      "Symbol Present: tensor([[1.]], device='cuda:0')\n",
      "Bounding Box Coordinates: (51.851589381694794, 67.03607633709908, 260.4733957052231, 278.98527562618256)\n",
      "Presence_predtensor([[0.9999]], device='cuda:0'), Bbox_predtensor([[231.9053,  22.7827, 187.0253, 108.8290]], device='cuda:0')\n",
      "Symbol Present: tensor([[1.]], device='cuda:0')\n",
      "Bounding Box Coordinates: (938.4917151927948, 57.49062222242355, 756.867821931839, 274.62330120801926)\n",
      "Presence_predtensor([[1.0000]], device='cuda:0'), Bbox_predtensor([[ 13.0618, 232.4523,  65.9376, 149.5340]], device='cuda:0')\n",
      "Symbol Present: tensor([[1.]], device='cuda:0')\n",
      "Bounding Box Coordinates: (52.85944487154484, 586.5788918733597, 266.84112906455994, 377.33965599536896)\n",
      "Presence_predtensor([[1.0000]], device='cuda:0'), Bbox_predtensor([[ 12.9643, 231.7199,  65.6700, 149.0401]], device='cuda:0')\n",
      "Symbol Present: tensor([[1.]], device='cuda:0')\n",
      "Bounding Box Coordinates: (52.465079724788666, 584.7306710481644, 265.75824296474457, 376.09326207637787)\n",
      "Presence_predtensor([[1.0000]], device='cuda:0'), Bbox_predtensor([[ 13.3742,  23.2324,  63.1057, 108.9346]], device='cuda:0')\n",
      "Symbol Present: tensor([[1.]], device='cuda:0')\n",
      "Bounding Box Coordinates: (54.123765259981155, 58.62559826672077, 255.38074660301208, 274.8895990252495)\n",
      "Presence_predtensor([[0.9999]], device='cuda:0'), Bbox_predtensor([[244.7360, 229.7761, 196.9732, 154.9321]], device='cuda:0')\n",
      "Symbol Present: tensor([[1.]], device='cuda:0')\n",
      "Bounding Box Coordinates: (990.4160311222076, 579.8256855010986, 797.1258795261383, 390.96158254146576)\n",
      "Presence_predtensor([[1.0000]], device='cuda:0'), Bbox_predtensor([[ 13.2985,  23.5516,  63.2693, 109.1953]], device='cuda:0')\n",
      "Symbol Present: tensor([[1.]], device='cuda:0')\n",
      "Bounding Box Coordinates: (53.81746006011963, 59.43108081817627, 256.04286539554596, 275.5474693775177)\n"
     ]
    }
   ],
   "source": [
    "# general_test(model)\n",
    "general_test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# image_path = 'front.bmp'\n",
    "# image_size = 256\n",
    "# image = Image.open(image_path).convert('RGB')\n",
    "# original_size = image.size\n",
    "# transform = transforms.Compose([\n",
    "#         transforms.Resize((image_size, image_size)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "# image = transform(image).unsqueeze(0).cuda()\n",
    "# with torch.no_grad():\n",
    "#     presence_pred, bbox_pred = model(image)\n",
    "#     print(f'Presence_pred{presence_pred}, Bbox_pred{bbox_pred}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = 0.5\n",
    "# symbol_present = (presence_pred > threshold).float()\n",
    "# print(f'Symbol Present: {symbol_present}')\n",
    "\n",
    "# image_cv = cv2.imread(image_path)\n",
    "# # image_cv = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for visualization\n",
    "# image_cv = np.array(image_cv)\n",
    "\n",
    "# if symbol_present.item() == 1:\n",
    "#     prediced_boxes = bbox_pred[0].cpu().numpy()\n",
    "#     # Extract bounding box coordinates\n",
    "#     x_min, y_min, x_max, y_max = prediced_boxes\n",
    "#     x_min = x_min * original_size[0] / image_size\n",
    "#     y_min = y_min * original_size[1] / image_size\n",
    "#     x_max = x_max * original_size[0] / image_size\n",
    "#     y_max = y_max * original_size[1] / image_size\n",
    "#     print(f\"Bounding Box Coordinates: {x_min, y_min, x_max, y_max}\")\n",
    "\n",
    "#     # Draw the bounding box on the image\n",
    "#     cv2.rectangle(image_cv, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 0, 0), 2)\n",
    "#     cv2.imshow('Augmented Image', image_cv)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "# else:\n",
    "#     center = (original_size[0] // 2, original_size[1] // 2)\n",
    "#     cv2.putText(image_cv, \"No Symbol Detected\", center, cv2.FONT_HERSHEY_SIMPLEX, 1, (125, 0, 0), 2)\n",
    "#     cv2.imshow('Augmented Image', image_cv)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "import onnx, onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "batch_size = 1\n",
    "image_size = 256\n",
    "dummy_input = torch.randn(batch_size, 3, image_size, image_size)\n",
    "dummy_input = dummy_input.to('cuda')\n",
    "torch.onnx.export(model,\n",
    "                  dummy_input,\n",
    "                  \"onnx_model.onnx\",\n",
    "                  export_params=True,\n",
    "                  opset_version=10, # the ONNX version to export the model to\n",
    "                  do_constant_folding=True, # whether to execute constant folding for optimization\n",
    "                  input_names=['image'],\n",
    "                  output_names=['presence_pred', 'bbox_pred'], \n",
    "                  dynamic_axes={'image' : {0 : 'batch_size'},    # variable lenght axes\n",
    "                                'presence_pred' : {0 : 'batch_size'},\n",
    "                                'bbox_pred' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Check Onnx Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"onnx_model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "torch_presence, torch_bbox = model(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) compare with Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "ort_session = onnxruntime.InferenceSession(\"onnx_model.onnx\", provider_options={'device_type': 'cuda'})\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(dummy_input)}\n",
    "ort_presence, ort_bbox = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "np.testing.assert_allclose(to_numpy(torch_presence), ort_presence, rtol=1e-03, atol=1e-05)\n",
    "np.testing.assert_allclose(to_numpy(torch_bbox), ort_bbox, rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) test onnx model using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.9999834]], dtype=float32), array([[ 12.866848,  25.555843,  64.27002 , 110.729385]], dtype=float32)]\n",
      "Bounding Box Coordinates: (52.07052546739578, 64.48857346177101, 260.09273529052734, 279.41868340969086)\n"
     ]
    }
   ],
   "source": [
    "image_path = 'back.bmp'\n",
    "image = Image.open(image_path) # RGB\n",
    "\n",
    "# print(image.size)\n",
    "# image_array = np.array(image)\n",
    "# print(image_array.shape)\n",
    "# for i in range(10):\n",
    "#     for j in range(10):\n",
    "#         print(f\"Index {i, j}: {image_array[i, j]}\")\n",
    "\n",
    "original_size = image.size\n",
    "image_size = 256\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "image = transform(image) # C x W x H\n",
    "# image = image.permute(1, 2, 0)  # Convert image from C x W x H to W x H x C\n",
    "# image = image.numpy()  # Convert image to numpy array\n",
    "# image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Convert RGB image to BGR\n",
    "# image = torch.from_numpy(image)  # Convert image back to torch tensor\n",
    "# image = image.permute(2, 0, 1)  # Convert image from W x H x C to C x W x H\n",
    "# print(image.shape)\n",
    "# for i in range(10):\n",
    "#     for j in range(10):\n",
    "#         for k in range(3):\n",
    "#             print(f\"Index {k, i, j}: {image[k, i, j]}\")\n",
    "    \n",
    "\n",
    "\n",
    "image = image.unsqueeze(0).cuda() # 1 x C x W x H\n",
    "\n",
    "\n",
    "\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(image)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "print(ort_outs)\n",
    "\n",
    "image_cv = cv2.imread(image_path)\n",
    "# image_cv = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for visualization\n",
    "image_cv = np.array(image_cv)\n",
    "\n",
    "# Extract bounding box coordinates\n",
    "presence = ort_outs[0][0]\n",
    "x_min, y_min, x_max, y_max = ort_outs[1][0]\n",
    "\n",
    "x_min = x_min * original_size[0] / image_size\n",
    "y_min = y_min * original_size[1] / image_size\n",
    "x_max = x_max * original_size[0] / image_size\n",
    "y_max = y_max * original_size[1] / image_size\n",
    "\n",
    "print(f\"Bounding Box Coordinates: {x_min, y_min, x_max, y_max}\")\n",
    "# Draw the bounding box on the image\n",
    "if presence > 0.5:\n",
    "        cv2.rectangle(image_cv, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 0, 0), 2)\n",
    "        cv2.imshow('Augmented Image', image_cv)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# image_size = 256\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#         # transforms.Resize((image_size, image_size)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "# # Test the model\n",
    "# image = Image.open(image_path).convert('RGB')\n",
    "# image = transform(image)\n",
    "\n",
    "# image = image.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)\n",
    "# image = image.to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = model(image)\n",
    "\n",
    "# prediced_boxes = output[0].cpu().numpy()\n",
    "# print(f\"Predicted Boxes: {prediced_boxes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the image to OpenCV format for visualization\n",
    "# image_cv = cv2.imread(image_path)\n",
    "# # image_cv = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for visualization\n",
    "# image_cv = np.array(image_cv)\n",
    "\n",
    "# # Extract bounding box coordinates\n",
    "# x_min, y_min, x_max, y_max = prediced_boxes\n",
    "# x_min = x_min * original_size[0] / image_size\n",
    "# y_min = y_min * original_size[1] / image_size\n",
    "# x_max = x_max * original_size[0] / image_size\n",
    "# y_max = y_max * original_size[1] / image_size\n",
    "# print(f\"Bounding Box Coordinates: {x_min, y_min, x_max, y_max}\")\n",
    "\n",
    "# # Draw the bounding box on the image\n",
    "# cv2.rectangle(image_cv, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 0, 0), 2)\n",
    "# cv2.imshow('Augmented Image', image_cv)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "springv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
